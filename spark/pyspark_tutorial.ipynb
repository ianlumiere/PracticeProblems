{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "\n",
    "# We have access to Spark through the variable sc\n",
    "nums = list(range(0, 1000001)) # this is on the driver machine\n",
    "\n",
    "# it is called driver because it will tell the other machines what to do\n",
    "# what spark will do is distribute this data across our other machines or cluster\n",
    "# of machines that will do the processing for us to speed it up. cluster means\n",
    "# your group of machines that are dedicated to performing this task.\n",
    "\n",
    "# first thing we need to do is to tell to distribute what data\n",
    "# normally you would load from a file, but for this tutorial we will just \n",
    "# use the list we made\n",
    "\n",
    "# parallelize takes in a python list and distributes it into an RDD \n",
    "# (resilent distributed dataset). Every item in an RDD can be a\n",
    "# number, list, tuple\n",
    "\n",
    "nums_rdd = sc.parallelize(nums) # this distributes the data and returns an RDD to nums_rdd\n",
    "\n",
    "nums_rdd.collect() # returns a python list of all of our info\n",
    "# scary operation to do this because all of this data is distributed across our many\n",
    "# different machines\n",
    "\n",
    "# nums_rdd.take() takes a number of how many things we want to take\n",
    "# this is safer than using collect()\n",
    "nums_rdd.take(5) # this takes the first five elements of the nums_rdd RDD\n",
    "# and returns it as a python list of the first five items\n",
    "\n",
    "# if you want to apply a function to every element in the RDD, you would do:\n",
    "squared_nums_rdd = nums_rdd.map(lambda x: x ** 2)\n",
    "# .map() maps a function to every element in the RDD\n",
    "# you can pass map a lambda function or a named function\n",
    "\n",
    "# if we wanted to make every element a tuple where the first is the number\n",
    "# and the second value is the number of digits, we would do:\n",
    "pairs = nums_rdd.map(lambda x: (x, len(str(x))))\n",
    "pairs.take(25) # will show us that this worked\n",
    "\n",
    "# map is awesome, we will use it all the time, but one thing it does not do \n",
    "# is remove things from your RDD. To do that, we use filter\n",
    "# So imagine we only want numbers with an even number of digits, we would do:\n",
    "even_pairs = pairs.filter(lambda x: (x[1] % 2) == 0)\n",
    "# just like map, we pass filter a function, but this time it returns a true or false\n",
    "# the ones that evaluate to true will be included, and the false ones will be removed\n",
    "# REMEMEBER: x right now is a tuple, so x[0] is the number and x[1] is the number of digits\n",
    "even_pairs.take(25) # will show us that this worked\n",
    "\n",
    "# to group info, we use \n",
    "# this is useful for aggregations, min/max, averages\n",
    "# let's group all the digit values that are 2s, 4s, 6s, and so on and then we want to\n",
    "# compute the average for each group (so we want only a few elements, one average\n",
    "# for the 2s, one average for the 4s, and so on), we would first swap the tuple so that\n",
    "# the group is the first item in the list. This is because spark has the notion of\n",
    "# key value pairs, where the key is the first element:\n",
    "swapped_pairs = even_pairs.map(lambda x: (x[1], x[0]))\n",
    "# then we want to group by the key\n",
    "grouped = swapped_pairs.groupByKey() # no need to pass anything because it will automatically\n",
    "# group by the key\n",
    "grouped.take(25) # notice we can have a bigger number than the number of elements and it \n",
    "# will not error. So even though we pass 25 and there are around 4 elements, it is no problem\n",
    "# looking at the output we see these weird pyspark objects as the value for each key,\n",
    "# to convert that, let's do\n",
    "grouped = grouped.map(lambda x: (x[0], list(x[1]))) # this will convert the pyspark object to a list\n",
    "grouped.take(25) # now it looks better\n",
    "# now lets average all the elements in the list at the second element\n",
    "averaged = grouped.map(lambda x: (x[0], sum(x[1]) / len(x[1])))\n",
    "averaged.collect() # can collect this since we only have a few\n",
    "\n",
    "# a better way to do this is to NOT use groupByKey. There is a faster way to do this\n",
    "# by using reduceByKey instead. THE EXAMPLE BELOW IS FROM A DIFFERENT VIDEO\n",
    "answer = grouped.reduceByKey(lambda x, y: x + y)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
